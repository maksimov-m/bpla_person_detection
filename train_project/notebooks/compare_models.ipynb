{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7e01d7-4c5f-491f-9a89-3438b49786ff",
   "metadata": {},
   "source": [
    "# В этом ноутбуке сравнивались обученные модели RetinaNet и YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f39f1d0-e57a-4179-8e92-cb98cd172baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from model import create_model\n",
    "from datasets import create_valid_dataset, create_valid_loader\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f550ed31-227e-4870-938c-5bb6c36837c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2762d5fa-607c-4abc-97f0-a8237949e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "checkpoint = torch.load(\"./best_retina_model.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(DEVICE).eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe3788b2-39ac-4734-b7d4-2ab7eee9539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_on_image(orig_image: np.ndarray, resize_dim=None, threshold=0.25):\n",
    "    \"\"\"\n",
    "    Runs inference on a single image (OpenCV BGR or NumPy array).\n",
    "    - resize_dim: if not None, we resize to (resize_dim, resize_dim)\n",
    "    - threshold: detection confidence threshold\n",
    "    Returns: processed image with bounding boxes drawn.\n",
    "    \"\"\"\n",
    "    image = orig_image.copy()\n",
    "    COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "    # Optionally resize for inference.\n",
    "    if resize_dim is not None:\n",
    "        image = cv2.resize(image, (resize_dim, resize_dim))\n",
    "\n",
    "    # Convert BGR to RGB, normalize [0..1]\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    # Move channels to front (C,H,W)\n",
    "    image_tensor = torch.tensor(image_rgb.transpose(2, 0, 1), dtype=torch.float).unsqueeze(0)\n",
    "    start_time = time.time()\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "    end_time = time.time()\n",
    "    # Get the current fps.\n",
    "    fps = 1 / (end_time - start_time)\n",
    "    fps_text = f\"FPS: {fps:.2f}\"\n",
    "    # Move outputs to CPU numpy\n",
    "    outputs = [{k: v.cpu() for k, v in t.items()} for t in outputs]\n",
    "    boxes = outputs[0][\"boxes\"].numpy()\n",
    "    scores = outputs[0][\"scores\"].numpy()\n",
    "    labels = outputs[0][\"labels\"].numpy().astype(int)\n",
    "\n",
    "    # Filter out boxes with low confidence\n",
    "    valid_idx = np.where(scores >= threshold)[0]\n",
    "    boxes = boxes[valid_idx].astype(int)\n",
    "    labels = labels[valid_idx]\n",
    "\n",
    "    h_orig, w_orig = orig_image.shape[:2]\n",
    "    \n",
    "    # If we resized for inference, rescale boxes back to orig_image size\n",
    "    if resize_dim is not None:\n",
    "        h_orig, w_orig = orig_image.shape[:2]\n",
    "        h_new, w_new = resize_dim, resize_dim\n",
    "        # scale boxes\n",
    "        boxes[:, [0, 2]] = (boxes[:, [0, 2]] / w_new) * w_orig\n",
    "        boxes[:, [1, 3]] = (boxes[:, [1, 3]] / h_new) * h_orig\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for box, label_idx in zip(boxes, labels):\n",
    "        class_name = CLASSES[label_idx] if 0 <= label_idx < len(CLASSES) else str(label_idx)\n",
    "        color = COLORS[label_idx % len(COLORS)][::-1]  # BGR color\n",
    "        cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), color, 5)\n",
    "        cv2.putText(orig_image, class_name, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)\n",
    "        cv2.putText(\n",
    "            orig_image,\n",
    "            fps_text,\n",
    "            (int((w_orig / 2) - 50), 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.8,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "    return orig_image, fps, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8bfad4c-2581-4155-acc9-b083d1f5baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_data_loader, model, metric):\n",
    "    print(\"Validating\")\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize tqdm progress bar.\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    target = []\n",
    "    preds = []\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "        # For mAP calculation using Torchmetrics.\n",
    "        #####################################\n",
    "        for i in range(len(images)):\n",
    "            true_dict = dict()\n",
    "            preds_dict = dict()\n",
    "            true_dict[\"boxes\"] = targets[i][\"boxes\"].detach().cpu()\n",
    "            true_dict[\"labels\"] = targets[i][\"labels\"].detach().cpu()\n",
    "            preds_dict[\"boxes\"] = outputs[i][\"boxes\"].detach().cpu()\n",
    "            preds_dict[\"scores\"] = outputs[i][\"scores\"].detach().cpu()\n",
    "            preds_dict[\"labels\"] = outputs[i][\"labels\"].detach().cpu()\n",
    "            preds.append(preds_dict)\n",
    "            target.append(true_dict)\n",
    "        #####################################\n",
    "    \n",
    "\n",
    "    metric.reset()\n",
    "    metric.update(preds, target)\n",
    "    metric_summary = metric.compute()\n",
    "    return metric_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d56d1591-999f-40f0-b84f-929c53ff1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "907b3cab-80bf-4605-b0ff-fea18ad71a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maks6\\Desktop\\Моя жизнь\\pet-projects\\БПЛА\\git\\retina_train\\venv\\lib\\site-packages\\albumentations\\core\\composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = create_valid_dataset(\"./data/lite_data/test/\")\n",
    "valid_loader = create_valid_loader(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25209beb-7788-4957-982c-913bb06ecff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:23<00:00, 11.88s/it]\n",
      "C:\\Users\\maks6\\Desktop\\Моя жизнь\\pet-projects\\БПЛА\\git\\retina_train\\venv\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "result = validate(valid_loader, model, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80a973ae-d666-476d-844b-46953a163806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': tensor(0.1746),\n",
       " 'map_50': tensor(0.4735),\n",
       " 'map_75': tensor(-1.),\n",
       " 'map_small': tensor(0.1777),\n",
       " 'map_medium': tensor(0.2141),\n",
       " 'map_large': tensor(-1.),\n",
       " 'mar_1': tensor(0.1198),\n",
       " 'mar_10': tensor(0.2656),\n",
       " 'mar_100': tensor(0.3490),\n",
       " 'mar_small': tensor(0.3333),\n",
       " 'mar_medium': tensor(0.4583),\n",
       " 'mar_large': tensor(-1.),\n",
       " 'map_per_class': tensor(-1.),\n",
       " 'mar_100_per_class': tensor(-1.),\n",
       " 'classes': tensor(1, dtype=torch.int32)}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a61fa1a7-56fb-4010-aebf-2bfa449ecd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.163  Python-3.10.8 torch-2.6.0+cpu CPU (AMD Ryzen 5 4600H with Radeon Graphics)\n",
      "YOLOv8s summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 450.468.6 MB/s, size: 5128.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\maks6\\Desktop\\Моя жизнь\\pet-projects\\БПЛА\\git\\retina_train\\lite_data_yolo_test\\labels... 30 imag\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\maks6\\Desktop\\ \\pet-projects\\\\git\\retina_train\\lite_data_yolo_test\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:19<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         30         66      0.641      0.333      0.367      0.124\n",
      "Speed: 7.2ms preprocess, 527.8ms inference, 0.0ms loss, 18.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_yolo = YOLO(\"./outputs/best_yolov8_model.pt\")\n",
    "result_yolo = model_yolo.val(data=\"./data.yaml\", iou=0.5, verbose=True, plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f76bad-0add-400d-af08-049ac1c81310",
   "metadata": {},
   "source": [
    "# В текущей реализации RetinaNet получила наилучший результат по метрике mAP50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5261ad-cb11-411d-b2f9-cba34e21020c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
